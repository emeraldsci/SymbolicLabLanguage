(* ::Package:: *)

(* ::Text:: *)
(*\[Copyright] 2011-2023 Emerald Cloud Lab, Inc.*)


(* ::Section:: *)
(* Description / Notes *)

(* --- AnalyzeLadder Overview --- *)
(*
	The goal of this function is to fit an equation to a set of peaks from some data or peaks object.

	Notable changes:
	(1) ResolvedOption ExpectedSizes will always have actual sizes, never a standard object, since Ladder will point to standard.
	(2) Added non-computable versions of ExpectedSize, ExpectedPosition called ExpectedSizeFunction, ExpectedPositionFunction.
*)

(* --- State Design --- *)
(*
State -> <|
	UnresolvedInputs -> ...
	ResolvedInputs -> ...
		* Resolved inputs does not need to match any function or object definitions. This can store whatever we need it to.
		Peaks, PeaksPositions, StandardSizes, LadderUnits, FitData, TransformationFunction
	UnresolvedOptions -> ...
	ResolvedOptions ->
		(Default Fields)
		Name, Output, Upload, Template,
		(Ladder Fields)
		ReferenceField, Align, Reverse, Function, FitType, ExpectedSizes, FitNormalization
	Tests ->
		InputTests, OptionTests
	Packet ->
		(Default Fields)
		Type, Name, UnresolvedOptions, ResolvedOptions, Reference, ReferenceField,
		(Ladder Fields)
		Sizes, SizeUnit, PositionUnit, FragmentPeaks, StandardFit, ExpectedSize, ExpectedPosition, PeaksAnalysis, Ladder
	Intermediate -> ...
|>
*)


(* ::Section:: *)
(* Source Code *)

(* ::Subsection:: *)
(* Input Pattern Definitions *)
ladderDataTypes = {
	Object[Data, Chromatography],
	Object[Data, PAGE],
	Object[Data, Western],
	Object[Data, AgaroseGelElectrophoresis],
	Object[Data, CapillaryGelElectrophoresisSDS]
};
ladderDataTypesP = ObjectP[ladderDataTypes];

ladderAnalysisTypes = {
	Object[Analysis, Peaks]
};
ladderAnalysisTypesP = ObjectP[ladderAnalysisTypes];

ladderAllInputTypesP = Join[
	ladderDataTypesP,
	ladderAnalysisTypesP
];

(* Option Pattern Definitions *)
expectedSizeTypes = {
	Object[Sample],
	Model[Sample, StockSolution, Standard]
};
expectedSizeTypesP = ObjectP[expectedSizeTypes];

(* ::Subsection:: *)
(* AnalyzeLadderOptions *)
DefineOptions[AnalyzeLadder,
	Options :> {
		IndexMatching[
		{
			OptionName -> ReferenceField,
			Default -> Automatic,
			Description -> "If given a data object as input, the peak positions used for ladder curve fitting are taken from the most recent peaks analysis associated with the ReferenceField in the source data object.",
			ResolutionDescription -> "If Automatic then picks a pre-specified field based on the object type.",
			AllowNull -> True,
			Widget -> Widget[Type->Expression, Pattern:> FieldP[Output->Short], Size->Word]
		},
		{
			OptionName -> Align,
			Default -> Minimum,
			Description -> "If Minimum, associate ladder with peaks starting from the minimum peak position.  If Maximum, associate starting from the maximum.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:> (Minimum | Maximum)]
		},
		{
			OptionName -> Reverse,
			Default -> Automatic,
			Description -> "If True, reverse the list of ladder sizes when associating with peaks. Defaults to False, except for Object[Data, PAGE] which defaults to True.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:> BooleanP]
		},
		{
			OptionName -> Function,
			Default -> Size,
			Description -> "If Size, fits to size as a function of position. If Position, fits to position as a function of size.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:> (Size | Position)]
		},
		{
			OptionName -> FitType,
			Default -> Exponential,
			Description -> "Type of fit to use for fitting ExpectedPosition and ExpectedSize.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:> (Linear | Cubic | Exponential)]
		},
		{
			OptionName -> ExpectedSizes,
			Default -> Automatic,
			Description -> "The ladder sizes to be associated with the peaks.",
			ResolutionDescription -> "Defaults to ladder information associated with input data's protocol, and defaults to a standard nucleotide ladder otherwise.",
			AllowNull -> False,
			Widget -> Alternatives[
				Widget[Type->Object, Pattern:>ObjectP[{Model[Sample,StockSolution,Standard], Object[Sample]}]],
				Adder[Widget[Type -> Quantity, Pattern :> GreaterP[0 Nucleotide], Units -> Nucleotide]],
				Adder[Widget[Type -> Quantity, Pattern :> GreaterP[0 Dalton], Units -> Dalton]]
			]
		},
		{
			OptionName -> Name,
			Default -> Null,
			Description -> "Name to be used as the name of Object[Analysis, Ladder] generated by the analysis.",
			AllowNull -> True,
			Category -> "General",
			Widget -> Widget[Type->Expression,Pattern:>_String, Size -> Line]
		},
		{
			OptionName -> PlotType,
			Default -> All,
			Description -> "Data to display on the preview plot.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:>Alternatives[All,Fit,Peaks]]
		},
		{
			OptionName -> FitNormalization,
			Default -> Automatic,
			Description -> "Indicates whether to normalize the coefficient of the fit expression for the Exponential, Logistic, LogisticBase10, or GeneralizedLogistic fitting.",
			AllowNull -> False,
			Widget -> Widget[Type->Enumeration, Pattern:>Alternatives[True,False]]
		},

		IndexMatchingInput -> "Input Data"

		],

		OutputOption,
		UploadOption,
		AnalysisTemplateOption
	}
];


(* ::Subsection:: *)
(* Warnings and Errors *)
Error::NoPickedPeaks="The peaks object `1` did not contain any peaks.";
Error::NoRelativeMigrationPeaks="No relative migration peaks were found in RelativeMigrationPeaksAnalyses field of input CESDS data object `1`. Please run the peak picking analysis by calling AnalyzePeaks on input with ReferenceField->RelativeMigrationData.";
Error::LadderMWNotFound="Input `1` could not be matched to the LadderAnalyteMolecularWeights field of its parent protocol (`2`). Please ensure this field contains ladder molecular weights.";
Error::AnalyzePeaksFailure="The `1` Field of `2` did not contain a peaks objects matching the requested reference field, `3`. AnalyzeLadder made the function call: AnalyzePeaks[`2`, ReferenceField\[Rule]`3`, Upload\[Rule]`4`], but errors were found during it's evaluation.";
Error::SingularPeak="The data object, `1`, contains only one peak, and cannot be properly fit to an equation.";
Warning::PeaksFieldEmpty="The `1` Field of `2` did not contain a peaks objects matching the requested reference field, `3`. Running AnalyzePeaks.";
Warning::UnknownUnits="Unknown standard units for `1`.";
Warning::DefaultSizes="Ladder sizes cannot be found in the input object, defaulting to a ladder composition with standard sizes of 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100 base pairs.";


(* ::Subsection:: *)
(* Main function *)
DefineAnalyzeFunction[
	AnalyzeLadder,
	<|
		InputData -> ladderAllInputTypesP
	|>,
	{
		Batch[resolveAnalyzeLadderDownloads], (* Batch for Downloads / Uploads of AnalyzePeaks. *)
		resolveAnalyzeLadderInputs,
		resolveAnalyzeLadderOptions,
		resolveAnalyzeLadderFitParameters,
		Batch[calculateLadderFields], (* Batch for Uploads of AnalyzeFit. *)
		analyzeLadderPreview
	}
];


(* ::Subsection:: *)
(* Download *)

(*
  N.B. In the helper functions, there is a lot of conversion between various held forms:
    Hold[{}] <> Hold[] <> {Hold[]..}
  This is so that we can properly use Join[] on held expressions and head swapping for Download calls.
  Similarly, most of this is done via rule replacement rather than other MM functions to avoid any evaluation of the
  underlying expressions, which is particularly important for fields with part specifications.
*)

(* ::Subsubsection:: *)
(*resolveAnalyzeLadderDownloads*)
resolveAnalyzeLadderDownloads[
	KeyValuePattern[{
		UnresolvedInputs -> KeyValuePattern[{InputData -> inputs_}],
		(* Safe ResolvedOptions, not fully ResolvedOptions. *)
		ResolvedOptions -> KeyValuePattern[{
			ReferenceField -> referenceFields_,
			ExpectedSizes -> expectedSizes_,
			Upload -> upload_,
			Name -> name_
		}],
		Batch -> True
	}]
	] := Module[
		{
			(* Pre-Download Variables. *)
			referenceFieldsPartiallyResolved,
			expectedSizesObjects, expectedSizeReferenceFields,
			downloadObjects, downloadReferenceFields,

			(* Download Result Variables. *)
			downloadResults, inputDownloadResults, expectedSizeDownloadResults, downloadResultsTransposed,

			(* Data Filtering / Reorganization Variables. *)
			analysisObjects, analysisReferenceFields, peaksData,
			dataObjects, protocolStuff, moleculeStuff,

			analysisObjectsFiltered, analysisReferenceFieldsFiltered, peaksDataFiltered,
			dataObjectsFiltered, protocolStuffFiltered, moleculeStuffFiltered,

			(* Analyze Peaks Variables. *)
			dataObjectsForAnalyzePeaks, dataRefFieldsForAnalyzePeaks,
			runPeaksAnalysis, dataObjectsIndiciesForAnalyzePeaks,
			newAnalysisObjects, newAnalysisReferenceFields, newPeaksData,

			(* Output / Packet Variables *)
			analysisObjectsFinal, analysisReferenceFieldsFinal, peaksDataFinalAsList, peaksDataFinalAsAssociation
		},

		(* --- Prepare Download Objects and Fields / Download from database --- *)
		(*
			During downloads, we must download fields from both the original objects, and from fields in linked objects.
			Which linked fields/objects we download from is partially dependent on the reference field, thus the reference
			fields here are used to determine what gets joined with hard-coded fields during the generateDownloadFields step
			of downloadLadderInputs. (e.g., Which Peaks Analysis field(s) do we download from? )
			For the expectedSize objects, we do not download through links, nor do we download from any linked fields within
			them, but must pad the list of reference fields with Null to have matching list lengths to MapThread when
			generating the full list of download fields.
		*)
		(* Partially resolve input referenceFields for data objects. *)
		(* Combine with reference fields for Analysis objects after downloading. *)
		referenceFieldsPartiallyResolved = infoPeaksField[inputs, referenceFields];

		(* Remove non-objects from expectedSize Objects. Create list of Nulls to Pad ReferenceField list for downloads. *)
		(*
			For objects, Null downloads nothing, but makes parsing data convenient.
			For reference fields, Null (with an actual obejct) does download from Constellation.
		*)
		expectedSizesObjects = Map[If[MatchQ[#, expectedSizeTypesP], #, Null]&, expectedSizes];
		expectedSizeReferenceFields = ConstantArray[Null, Length[expectedSizesObjects]];

		(* Join all download objects and reference fields *)
		downloadObjects = Join[inputs,expectedSizesObjects];
		downloadReferenceFields = Join[referenceFieldsPartiallyResolved, expectedSizeReferenceFields];

		(* Generate full list of download fields and download results. *)
		(*
			Again, Null reference fields does not mean no download, just that we are not downloading through reference-field
			-dependent linked fields. Only Null object corresponds to no download.
		*)
		downloadResults = downloadLadderInputs[downloadObjects, downloadReferenceFields];


		(* --- Split the download results into convenient bundles of data --- *)
		(* Split download results into input downloads and expectedSize downloads. *)
		inputDownloadResults = downloadResults[[;;Length[inputs]]];
		expectedSizeDownloadResults = downloadResults[[Length[inputs]+1;;]];
		downloadResultsTransposed = Transpose[inputDownloadResults];

		(* Further split inputDownloadResults. *)
		(* Fields from Analysis Objects. *)
		analysisObjects = downloadResultsTransposed[[1]];
		analysisReferenceFields = downloadResultsTransposed[[2]];
		peaksData = Transpose[downloadResultsTransposed[[3;;34]]];
		(* Field from Data Objects. *)
		dataObjects = downloadResultsTransposed[[35]];
		protocolStuff = Transpose[downloadResultsTransposed[[36;;38]]];
		moleculeStuff = downloadResultsTransposed[[39]];


		(* --- Filter the Data --- *)
		{
			analysisObjectsFiltered, analysisReferenceFieldsFiltered, peaksDataFiltered,
			dataObjectsFiltered, protocolStuffFiltered, moleculeStuffFiltered,
			dataObjectsForAnalyzePeaks, dataRefFieldsForAnalyzePeaks
		} = Transpose[
			MapThread[filterDownloadResults[#1, #2, #3, #4, #5, #6, #7, #8]&,
				{inputs, referenceFieldsPartiallyResolved, analysisObjects, analysisReferenceFields, peaksData, dataObjects, protocolStuff, moleculeStuff}
			]
		];

		(* --- Run additional peaks analyses if needed --- *)
		(*
			Before removing Nulls, create list of booleans for whether AnalyzePeaks run on input.
			Used to check for failed peaks analysis in following step.
		*)
		runPeaksAnalysis = Map[!MatchQ[#, Null]&, dataObjectsForAnalyzePeaks];

		(* Determine indicies of inputs requiring peaks analyses. Used to join new and old peaks data. *)
		dataObjectsIndiciesForAnalyzePeaks = Flatten[Position[runPeaksAnalysis, True]];

		(* Remove Nulls from dataObjectsForAnalyzePeaks and dataRefFieldsForAnalyzePeaks. *)
		dataObjectsForAnalyzePeaks = Delete[dataObjectsForAnalyzePeaks, Position[dataObjectsForAnalyzePeaks, Null]];
		dataRefFieldsForAnalyzePeaks = Delete[dataRefFieldsForAnalyzePeaks, Position[dataRefFieldsForAnalyzePeaks, Null]];

		(* Perform AnalyzePeaks on specific Data Objects, if needed. *)
		{newAnalysisObjects, newAnalysisReferenceFields, newPeaksData}
				= runAdditionalPeaksAnalyses[dataObjectsForAnalyzePeaks, dataRefFieldsForAnalyzePeaks, upload];


		(* --- Join new data with old data --- *)
		(* Note, because of how data filtering is done, the final data sets are effectively un-transposed, and work natively with Batch. *)
		analysisObjectsFinal = ReplacePart[analysisObjectsFiltered,
			Thread[dataObjectsIndiciesForAnalyzePeaks -> newAnalysisObjects]];

		analysisReferenceFieldsFinal = ReplacePart[analysisReferenceFieldsFiltered,
			Thread[dataObjectsIndiciesForAnalyzePeaks -> newAnalysisReferenceFields]];

		peaksDataFinalAsList = ReplacePart[peaksDataFiltered,
			Thread[dataObjectsIndiciesForAnalyzePeaks -> newPeaksData]];

		peaksDataFinalAsAssociation = Map[
			If[!MatchQ[#, $Failed],
				Association[Thread[PeaksFields->#]],
				#
			]&,
			peaksDataFinalAsList
		];


		(* ----- Return Association ----- *)
		Association[
			ResolvedOptions -> <|
				ReferenceField -> analysisReferenceFieldsFinal
			|>,
			Intermediate -> <|
				AnalysisObject -> analysisObjectsFinal,
				ReferenceField -> analysisReferenceFieldsFinal,
				PeaksData -> peaksDataFinalAsAssociation,
				DataObject -> dataObjectsFiltered,
				ProtocolInfo -> protocolStuffFiltered,
				MoleculeInfo -> moleculeStuffFiltered,
				ExpectedSizeInfo -> expectedSizeDownloadResults,
				RunPeaksAnalysis -> runPeaksAnalysis
			|>,
			Batch -> True
		]
	];


(* ::Subsubsection:: *)
(* removeNestedHolds *)
(* Removes nested Holds without evaluating any of the expression. Returns a Held expression. *)
removeNestedHolds[expr_Hold] := Replace[expr, Hold[blah___] :> blah, {2, Infinity}, Heads -> True];


(* ::Subsubsection:: *)
(* appendReference *)
(* List option must come first for proper behavior, or pattern of second definition must exclude lists. *)
appendReference[Hold[reference_], Hold[heldList_List] ] := Module[
	{listOfHolds, appendedList},

	(* Convert the held list of download fields into a list of individual holds. *)
	listOfHolds = Replace[Hold[heldList], blah_ :> Hold[blah], {2}] /. Hold[{blah__}] :> {blah};
	(* Recursively call AppendReference as needed to generate a list of Held expression. *)
	(* This completely flattens the list! Be mindful of this when unpacking download results. *)
	appendedList = Map[appendReference[Hold[reference], #] &, listOfHolds];
	(* Convert list of holds into a large held expression. *)
	appendedList /. {blah__} :> Join[blah]
];

appendReference[Hold[reference_], Hold[symbol1_[symbol2___]]] := Module[{},
	(* Use AppendReference recursively to apply Hold to only the outermost part of the expression. *)
	With[
		{evaluatedPortion = appendReference[Hold[reference], Hold[symbol1]]},
		removeNestedHolds[Hold[evaluatedPortion[symbol2]]]
	]
];

appendReference[Hold[reference_], Hold[symbol1_[[symbol2___]]]] := Module[{},
	(* Use AppendReference recursively to apply Hold to only the outermost part of the expression,
  applying special care to expression with Part specifications (e.g. expr[[1]]). *)
	With[
		{evaluatedPortion = appendReference[Hold[reference], Hold[symbol1]]},
		removeNestedHolds[Hold[Part[evaluatedPortion, symbol2]]]
	]
];

(* Final definition for AppendReference / Ends Recursion. Applies hold to outermost part of expression. *)
appendReference[Hold[reference_], Hold[symbol1_] ] := removeNestedHolds[Hold[reference[symbol1]]];


(* ::Subsubsection:: *)
(* generateDownloadFields *)

(* Define the download fields for all input types, assuming direct download. *)
(* Use with/hold to expand the PeaksField within the held expression. *)
$analysisFields = With[
	{peaksFieldsExpanded = PeaksFields},
	Hold[{Object, ReferenceField, peaksFieldsExpanded}]
];
$dataFields = Hold[{
	Object,
	Protocol[Object],
	Protocol[Ladders][Object],
	Protocol[LadderAnalyteMolecularWeights],
	SamplesIn[Model][Composition][[All, 2]][Molecule]
}];

(* Input Definitions *)
generateDownloadFields[in:ladderDataTypesP, referenceField_] := Module[
	{
		inputType, analysisReferenceField, allFields,
		analysisFieldsModified, dataFieldsFlat, dataFieldsModified
	},

	inputType = Download[in, Type];
	analysisReferenceField = infoPeaksField[inputType, referenceField];
	With[
		(* Determine the field pointing to our Analysis Objects. *)
		(* peaksFieldToPeaksSourceField matches on TypeP, not ObjectP. *)
		{analysisReferenceSourceField = peaksFieldToPeaksSourceField[inputType, analysisReferenceField]},

		(* Append analysisReferenceField to each of the individual analysisFields. *)
		analysisFieldsModified = appendReference[Hold[analysisReferenceSourceField], $analysisFields];

		(* Flatten DataFields and update from Hold[{}] to Hold[] so we can use Join[]. *)
		(* Note that we use replacement rules to completely flatten the list while avoiding evaluation. *)
		dataFieldsFlat = Replace[$dataFields, {blah__} :> blah, {2, Infinity}];
		dataFieldsModified = dataFieldsFlat /. Hold[{blah__}] :> Hold[blah];

		(* Join all fields and convert back into a held list for use with download. *)
		allFields = Join[analysisFieldsModified, dataFieldsModified ] /.Hold[blah__] :> Hold[{blah}]
	]
];
generateDownloadFields[in:ladderAnalysisTypesP, referenceField_] := Module[
	{
		inputType, allFields,
		analysisFieldsModified, analysisFieldsFlat, dataFieldsModified
	},

	inputType = Download[in, Type];
	With[{dataReferenceField = Reference},
		analysisFieldsFlat = Replace[$analysisFields, {blah__} :> blah, {2, Infinity}];
		analysisFieldsModified = analysisFieldsFlat /. Hold[{blah__}] :> Hold[blah];
		dataFieldsModified = appendReference[Hold[dataReferenceField], $dataFields];
		allFields = Join[analysisFieldsModified, dataFieldsModified] /. Hold[blah__] :> Hold[{blah}]
	]
];

(* ExpectedSize Definitions *)
generateDownloadFields[in:ObjectP[Model[Sample,StockSolution,Standard]], referenceField_] := Hold[{Object, StandardLengths}];
generateDownloadFields[in:ObjectP[Object[Sample]], referenceField_] := Hold[{Model[Object], Model[Object][StandardLengths]}];
generateDownloadFields[in:Null, referenceField_] := Hold[{Null, Null}];

(* List Definition. *)
generateDownloadFields[in:{(ladderAllInputTypesP|expectedSizeTypesP|Null)..}, referenceFields_List] := Module[
	{allFieldsList, allFieldsJoinList, allFieldsHeldList},

	(* Determine the correct list of download fields for all objects. *)
	(* Join all lists of fields under a single hold, and wrap result in list head for use with Download. *)
	allFieldsList = MapThread[generateDownloadFields[#1, #2]&, {in, referenceFields}];
	allFieldsJoinList = allFieldsList /. {blah__} :> Join[blah];
	allFieldsHeldList = allFieldsJoinList /. Hold[blah__] :> Hold[{blah}]
];


(* ::Subsubsection:: *)
(* downloadLadderInputs *)
downloadLadderInputs[in:{(ladderAllInputTypesP|expectedSizeTypesP|Null)..}, referenceFields_List] := Module[
	{heldInputs, heldFields},

	(* Convert input to Held List. *)
	heldInputs = Hold[in];

	(* Determine appropriate Download Fields. *)
	heldFields = generateDownloadFields[in, referenceFields];

	(* Download the results, quieting specific messages. *)
	Quiet[
		Download @@ Join[heldInputs, heldFields],
		{Download::FieldDoesntExist, Download::NotLinkField}
	]

];

(* ::Subsubsection:: *)
(* filterDownloadResults *)
(*
  PeaksSourceField in Data Objects can contain multiple AnalysisPeaks Objects,
  including those ran on data other than the desired reference field (mostly with Data,PAGE).
  We need to filter out downloaded AnalysisPeaks object(s) whose ReferenceField does not match
  the intended ReferenceField.
  If there are multiple valid Analysis,Peaks objects, use the most recent one.
  If there are no valid Analysis,Peaks objects, run AnalyzePeaks[].
*)

(*
  When input is an AnalysisPeaks Object, get basic data from the peaks object and last data object.
  Already has a valid peaks object, so set fields for AnalyzePeaks to Null.
*)
filterDownloadResults[
	(* General Inputs*) in:ladderAnalysisTypesP, referenceField_,
	(* Analysis Object Inputs *) analysisObject_, analysisReferenceField_, peakData_,
	(* Data Object Inputs *) dataObject_, protocol_, molecule_
] := {
	(* Analysis Object Info *) analysisObject, analysisReferenceField, peakData,
	(* Data Object Info *) Last[dataObject], Map[Last[#, Null]&, protocol], Last[molecule, Null],
	(* Analyze Peaks Info *) Null, Null
};

(*
	When input is a Data Object, get basic data from dat object, check for valid AnalysisPeaks objects.
*)
filterDownloadResults[
	(* General Inputs*) in:ladderDataTypesP, referenceField_,
	(* Analysis Object Inputs *) analysisObject_, analysisReferenceField_, peakData_,
	(* Data Object Inputs *) dataObject_, protocol_, molecule_
] := Module[{index},

	(* Determine the index of the most recent valid analysis object. *)
	index = Last[Position[analysisReferenceField, referenceField], {Null}][[1]];

	If[MatchQ[index, Null],
		(* True -> No valid Analysis Objects.*)
		{
			(* Analysis Object Info*) Null, Null, Null,
			(* Data Object Info *) dataObject, protocol, molecule,
			(* Analyze Peaks Info *) dataObject, referenceField
		},
		(* False -> One or More valid Analysis Objects.*)
		{
			(* Analysis Object Info*) analysisObject[[index]], analysisReferenceField[[index]], peakData[[All, index]],
			(* Data Object Info *) dataObject, protocol, molecule,
			(* Analyze Peaks Info *) Null, Null
		}
	]
];

(* ::Subsubsection:: *)
(* runAdditionalPeaksAnalyses *)

runAdditionalPeaksAnalyses[dataObjectsForAnalyzePeaks:{}, _, _] := {{}, {}, {}};
runAdditionalPeaksAnalyses[dataObjectsForAnalyzePeaks_, dataRefFieldsForAnalyzePeaks_, upload_] := Module[
	{
		newPeaksIDs, newPeaksAnalysesWithHeaders, newPeaksAnalyses,
		newAnalysisObjects, newAnalysisReferenceFields, newPeaksData
	},

	(* Throw Warning About Missing Peaks Objects. *)
	MapThread[Message[Warning::PeaksFieldEmpty, peaksFieldToPeaksSourceField[Download[#1,Type], #2], #1, #2]&,
		{dataObjectsForAnalyzePeaks, dataRefFieldsForAnalyzePeaks}
	];

	(* Create IDs *)
	newPeaksIDs = Table[CreateID[Object[Analysis, Peaks]], Length[dataObjectsForAnalyzePeaks]];

	(* Perform Peaks Analysis *)
	(*
		Since we are passing in rawData, AnalyzePeaks does not download, and can be ran in serial.
		Always return packets (Upload->False). Upload after, if requested.
		Use MapThread with Output->Result, not Output->{Result}, for correct listedness.
	*)
	newPeaksAnalysesWithHeaders = MapThread[AnalyzePeaks[
		#1,
		ReferenceField -> #2,
		Upload -> False,
		Output -> Result
	]&, {dataObjectsForAnalyzePeaks, dataRefFieldsForAnalyzePeaks}];

	(* Append Object IDs to packets. *)
	newPeaksAnalysesWithHeaders = MapThread[
		If[!MatchQ[#1, $Failed], Append[#1, Object-> #2], #1]&,
		{newPeaksAnalysesWithHeaders, newPeaksIDs}
	];

	(* Upload if requested. *)
	If[First[upload], Upload[newPeaksAnalysesWithHeaders]];

	(* Remove headers for internal use. *)
	newPeaksAnalyses = stripAppendReplaceKeyHeads[newPeaksAnalysesWithHeaders];

	(* Pull out results. *)
	(* Need to check for $Failed when parsing results. *)
	(* Handle framework-level individual $Failed cases at start of next main singleton step, resolveAnalyzeLadderInputs. *)
	newAnalysisObjects = Map[
		If[!MatchQ[#, $Failed],Lookup[#, Object, Null], #]&,
		newPeaksAnalyses
	];

	(* Return Original RefField in $Failed case for Error::AnalysisPeaksFailure Message. *)
	newAnalysisReferenceFields = MapThread[
		If[!MatchQ[#1, $Failed], Lookup[#1, ReferenceField, Null], #2]&,
		{newPeaksAnalyses, dataRefFieldsForAnalyzePeaks}
	];

	newPeaksData = Map[
		If[!MatchQ[#, $Failed], Lookup[#, PeaksFields, Null], #]&,
		newPeaksAnalyses
	];

	{newAnalysisObjects, newAnalysisReferenceFields, newPeaksData}

];


(* ::Subsection:: *)
(*Input and Option Resolution*)

(* ::Subsubsection:: *)
(*resolveAnalyzeLadderInputs*)
resolveAnalyzeLadderInputs[
	KeyValuePattern[{
		ResolvedOptions -> KeyValuePattern[{
			Name -> name_,
			Upload -> upload_,
			ReferenceField -> referenceField_
		}],
		Intermediate -> KeyValuePattern[{
			AnalysisObject -> analysisObject_,
			PeaksData -> peaksRaw_,
			DataObject -> dataObject_,
			RunPeaksAnalysis -> runPeaksAnalysis_
		}]
	}]
	] := Module[{
		dataObjectLink,peaks, order,  peaksWithoutNull, sortedPeaks, peaksPositions
	},

	(* Check results of Download / AnalyzePeaks. *)
	(* Peaks $Failed or Empty. *)
	If[MatchQ[peaksRaw, $Failed|Null|{}] || MatchQ[Lookup[peaksRaw,Position], $Failed|Null|{}],
		Which[

			(* Failed Peaks Analysis. *)
			MatchQ[runPeaksAnalysis, True],
				Message[Error::AnalyzePeaksFailure,
					peaksFieldToPeaksSourceField[Download[dataObject,Type], referenceField], dataObject, referenceField, upload
				],

			(* Peaks object exists, but missing peaks data. *)
			MatchQ[referenceField, RelativeMigrationData],
				Message[Error::NoRelativeMigrationPeaks, Download[input,Object]],

			(* Other *)
			True,
				Message[Error::NoPickedPeaks, analysisObject]

		];
		Return[$Failed]
	];

	(* If there is only 1 peak in AnalysisObject, AnalyzeFit will fail, exit early and Return[$Failed]. *)
	If[MatchQ[Length[Lookup[peaksRaw,Position]], 1],
		Message[Error::SingularPeak, dataObject];
		Return[$Failed]
	];

	(* Create Data Object Link. *)
	dataObjectLink = If[MatchQ[dataObject, ObjectP[Object[Data, Chromatography]]],
		Link[dataObject, StandardAnalyses],
		Link[dataObject, LadderAnalyses]
	];

	(* Minimal Pre-processing. *)
	(* Sort PeaksFields by peaks position, in case peaks are not in order due to being picked from multiple domains. *)
	peaks = If[MatchQ[peaksRaw, _Association],
		(
			(* Order peaks by position. *)
			order = Ordering[ Lookup[peaksRaw,Position,{}] ];

			(* Map conditional ordering function over peaksRaw Association, converting Null to {}. Convert result back to Association. *)
			(* #1=Key, #2=Value *)
			peaksWithoutNull = peaksRaw/.{Null->{}};
			sortedPeaks = Association@KeyValueMap[
				#1 -> If[MatchQ[Length[#2],Length[order]],
					#2[[order]], #2
				]&,
				peaksWithoutNull
			];
			sortedPeaks

		),
		Return[$Failed]
	];

	(* Get the sorted peaks positions. *)
	peaksPositions = peaks[Position];

	(* ----- Return Association ----- *)
	Association[
		ResolvedInputs -> <|
			DataObject -> dataObject,
			AnalysisObject -> analysisObject,
			Peaks -> peaks,
			PeaksPositions -> peaksPositions
		|>,
		ResolvedOptions -> <|
			ReferenceField-> referenceField
		|>,
		Tests -> <|
			ResolvedInputTests -> {}
		|>,
		Packet -> <|
			Type -> Object[Analysis, Ladder],
			Name -> name,
			Replace[Reference] -> dataObjectLink,
			Replace[ReferenceField] -> referenceField,
			(* Don't create the link only when Upload->False and a new peaks analysis was run /  not uploaded. *)
			If[!upload && runPeaksAnalysis, Nothing, Replace[PeaksAnalysis] -> Link[analysisObject, StandardAnalysis]]
		|>
	]

	];


(* ::Subsubsection:: *)
(*resolveAnalyzeLadderOptions*)
resolveAnalyzeLadderOptions[
	KeyValuePattern[{
		UnresolvedInputs -> KeyValuePattern[{InputData -> input_}],
		ResolvedOptions -> KeyValuePattern[{
			Reverse -> reverse_,
			FitType -> fitType_,
			ExpectedSizes -> expectedSizes_,
			FitNormalization -> fitNormalization_
		}],
		Intermediate -> KeyValuePattern[{
			ProtocolInfo -> protocolInfo_,
			MoleculeInfo -> moleculeInfo_,
			ExpectedSizeInfo -> objectAndStandardLengths_
		}]
	}]
	] := Module[{
		reverseResolved, fitNormalizationResolved,
		defaultSizes, expectedSizesResolved, molecules, strandLengths, ladderLink
	},

	(* Reverse *)
	(* If input matches Object[Data, PAGE] and Reverse is not specified, set Reverse->True as a default, else Reverse->False. *)
	reverseResolved =
  	If[MatchQ[reverse, Automatic],
			If[MatchQ[Object[Data,PAGE], Download[input,Type]],
				True, (* Reverse = Automatic, Input = Object[Data,Page] *)
				False (* Reverse = Automatic, Input =/= Object[Data,Page] *)
			],
			reverse (* Reverse =/= Automatic *)
		];

	(* FitNormalization *)
	(* If the fitting is exponential, use fit normalization *)
	fitNormalizationResolved = If[MatchQ[fitNormalization,Automatic],
		Switch[fitType, Exponential, True, _, False],
		fitNormalization
	];

	(* ExpectedSize *)
	(*
	If ExpectedSizes is Automatic, look for molecules and associated standard lengths/units in the data or
	analysis object, and returns the default values if none are found.
	Special case for when ExpectedSizes is Automatic and input is Object[Data, CapillaryGelElectrophoresisSDS].
	*)
	defaultSizes = {
		10*Nucleotide, 15*Nucleotide, 20*Nucleotide, 25*Nucleotide, 30*Nucleotide, 35*Nucleotide, 40*Nucleotide,
		45*Nucleotide, 50*Nucleotide, 60*Nucleotide, 70*Nucleotide, 80*Nucleotide, 90*Nucleotide, 100*Nucleotide
	};

	expectedSizesResolved = Which[
		(*Case 1 - ExpectedSizes is Automatic*)
		MatchQ[expectedSizes, Automatic],
			If[MatchQ[input,ObjectP[Object[Data,CapillaryGelElectrophoresisSDS]]],

				(* Case 1a - True *)
				(* Check ProtocolInfo for valid LadderWeights. *)
				(
					{protocol, ladders, ladderWeights} = protocolInfo;
					First[ladderWeights,
						(
							Message[Error::LadderMWNotFound, input, protocol];
							{}
						)
					]
				),

				(*Case 1b - False *)
				(*
					If there are valid strand lengths, sort and multiply by nucleotides, then return.
					Else, assume samples field is empty or corrupted, throw warning and return default sizes.
				*)
				(
					molecules = First[moleculeInfo, $Failed];
					strandLengths = DeleteCases[(StrandLength /@ molecules), StrandLength[_]];
					If[MatchQ[strandLengths, {_?NumericQ..}],
						Sort[strandLengths] * Nucleotide,
						Message[Warning::DefaultSizes];
						defaultSizes
					]
				)
			],

		(* Case 2 - ExpectedSizes is a valid Object or Model. *)
		MatchQ[expectedSizes, expectedSizeTypesP],
			objectAndStandardLengths[[2]],

		 (*Case 3 - List, Null, or $Failed*)
		MatchQ[expectedSizes, _List|Null|$Failed],
			expectedSizes

	];

	(* Make a link to the ExpectedSizes Object, or Null if a list/Automatic. *)
	ladderLink = If[MatchQ[expectedSizes, expectedSizeTypesP],
		Link[objectAndStandardLengths[[1]], LadderAnalyses],
		Null
	];

	(* ----- Return Association ----- *)
	Association[
		ResolvedOptions -> <|
			Reverse -> reverseResolved,
			FitNormalization -> fitNormalizationResolved,
			ExpectedSizes -> expectedSizesResolved
		|>,
		Tests -> <|
			ResolvedOptionTests -> {}
		|>,
		Packet -> <|
			Replace[Ladder] -> ladderLink
		|>
	]

	];


(* ::Subsubsection:: *)
(*resolveAnalyzeLadderFitParameters*)
(* This function does NOT perform AnalyzeFit. That happens in calculateLadderFields. *)
resolveAnalyzeLadderFitParameters[
	KeyValuePattern[{
		ResolvedInputs -> KeyValuePattern[{
			DataObject -> dataObject_,
			PeaksPositions -> peaksPositions_
		}],
		ResolvedOptions -> KeyValuePattern[{
			Align -> align_,
			Reverse -> reverse_,
			Function -> function_,
			ExpectedSizes -> expectedSizes_
		}]
	}]
	] := Module[{
		numPeaks, numTake,
		ladderUnits, sizeUnit, positionUnit, oldSizeUnit,
		standardLengths, transfFunc,
		fitData
	},

	(* Determine number of peaks to associate with standards, and whether peaks should be picked from
		smallest to largest (Align = Minimum) or largest to smallest (Align = Maximum). *)
	numPeaks = Min[{Length[expectedSizes], Length[peaksPositions]}];
	numTake = Switch[align,
		Minimum, numPeaks,
		Maximum, -numPeaks
	];

	(* --- SizeUnits & PositionUnits *)
	(* Determine Size/Position units based on the data object type. *)
	ladderUnits = analyzeLadderUnits[dataObject];
	{sizeUnit, positionUnit} = ladderUnits;

	(* Get the size unit from ExpectedSizes in ResolvedOptions (oldSizeUnit). *)
	oldSizeUnit = Units[First[expectedSizes]];

	(* --- Update Standard Lengths *)
	(* Get unitless lengths of the ladder fragments, and reverse them if requested. *)
	(* Assign units by comparing the size unit from ExpectedSizes in ResolvedOptions (oldSizeUnit) to units based on the DataObject.  *)
	standardLengths = Round@Take[Unitless[
		(* Values *) If[reverse, Reverse[expectedSizes], expectedSizes],
		(* Units *)  If[CompatibleUnitQ[oldSizeUnit,sizeUnit], sizeUnit, oldSizeUnit]
	], numTake];

	(* Define a (Transformation) function which ensures that the standard lengths and peaks have same number of data points. *)
	(* Use With since Attributes[Function] includes HoldAll. *)
	transfFunc = With[{numTake=numTake},Function[Take[#,numTake]]];

	(* Combine the peaks data and standard fit data *)
	fitData = Switch[function,
		Position,
		Transpose[{transfFunc[standardLengths],transfFunc[peaksPositions]}],
		Size,
		Transpose[{transfFunc[peaksPositions],transfFunc[standardLengths]}]
	];

	(* ----- Return Association ----- *)
	Association[
		ResolvedInputs -> <|
			FitData -> fitData,
			TransformationFunction -> transfFunc
		|>,
		Packet -> <|
			Replace[Sizes] -> standardLengths,
			Replace[SizeUnit] -> sizeUnit,
			Replace[PositionUnit] -> positionUnit
		|>
	]

	];



(* ::Subsection:: *)
(*Calculate*)
(* ::Subsubsection:: *)
(*calculateLadderFields*)
calculateLadderFields[
	KeyValuePattern[{
		UnresolvedInputs -> KeyValuePattern[{InputData -> input_}],
		ResolvedInputs -> KeyValuePattern[{
			FitData -> fitData_,
			TransformationFunction -> transfFunc_
		}],
		ResolvedOptions -> KeyValuePattern[{
			Reverse -> reverse_,
			FitType -> fitType_,
			FitNormalization  -> fitNormalization_,
			Function -> function_,
			Upload -> upload_
		}],
		Packet -> packet_,
		Batch -> True
	}]
	] := Module[{
		newPacket, sizeUnit, positionUnit,
		fitID, fitPackets, fitPacketsWithID, fragmentPeaks,
		fitExpressions, fitInverseExpressions,
		expectedSizeFit, expectedPositionFit
	},

	(* Strip Headers from Packet and Assign needed variables. *)
	newPacket = stripAppendReplaceKeyHeads[packet];
	sizeUnit = Lookup[newPacket, SizeUnit];
	positionUnit = Lookup[newPacket, PositionUnit];

	(*
		Run AnalyzeFit and return the packet. Quiet specific Messages.
		Set Upload -> False to return packets.

		Since we pass in raw data, set Upload->False, and AnalyzeFit does not download, we can MapThread over the function
		without wasting time. Additionally, at time of writing, AnalyzeFit does not support batch inputs.
	*)
	fitPackets = MapThread[
		Quiet[
			AnalyzeFit[#1, #2,
				{FitNormalization->#3,
				TransformationFunction->#4,
				ReferenceField->Position,
				Upload->False}
			],
			{NonlinearModelFit::cvmit}
		]&,
		{fitData, fitType, fitNormalization, transfFunc}
	];

	(* Create List of IDs for AnalyzeFit packets. Append to packets. *)
	fitID = Table[CreateID[Object[Analysis,Fit]], Length[input]];
	fitPacketsWithID = MapThread[
		Prepend[#1, Object->#2]&,
		{fitPackets, fitID}
	];

	(* Upload after calculating so that we can batch uploads.*)
	If[First[upload], Upload[fitPacketsWithID]];

	(* Not sure what this is doing other than returning fitData and potentially reversing it.
	Should drop the lookup and just use fit data directly. *)
	fragmentPeaks = MapThread[
		Switch[#1,
			Position, #2,
			Size, Map[Reverse, #2]
		]&,
		{function, Lookup[fitPacketsWithID,DataPoints]}
	];

	(* Generate a pure function from the fit packet. *)
  fitExpressions = Map[BestFitFunction/.#&, fitPacketsWithID];
	(* Generate a pure function which is the inverse of the above. *)
  fitInverseExpressions = Map[Quiet[InverseFunction[#]]&, fitExpressions];

	(* Use results from AnalyzeFit to define quantity functions. *)
	(* QuantityFunction returns a MM function, and is not evaluated unless given additional input as either:
		QuantityFunction[func, InputUnit, OutputUnit][input]
		   or
		func = QuantityFunction[func, InputUnit, OutputUnit]; func[input]
	*)
  unitizeFitFunctions[Size, fitExp_, fitInvExp_, sUnit_, pUnit_]:=
    {
	    QuantityFunction[fitExp, pUnit, sUnit],
	    QuantityFunction[fitInvExp, sUnit, pUnit]
    };
  unitizeFitFunctions[Position, fitExp_, fitInvExp_, sUnit_, pUnit_]:=
    {
	    QuantityFunction[fitInvExp, pUnit, sUnit],
	    QuantityFunction[fitExp, sUnit, pUnit]
    };

  {expectedSizeFit, expectedPositionFit} = Transpose[
		MapThread[unitizeFitFunctions[#1, #2, #3, #4, #5]&,
		{function, fitExpressions, fitInverseExpressions, sizeUnit, positionUnit}]
	];

	(* ----- Return Association ----- *)
	Association[
		Packet -> <|
			If[First[upload], Replace[StandardFit] -> Link[Lookup[fitPacketsWithID,Object],Reference], Nothing],
			Replace[FragmentPeaks] -> fragmentPeaks,
			Replace[ExpectedSizeFunction] -> expectedSizeFit,
			Replace[ExpectedPositionFunction] -> expectedPositionFit
		|>,
		Batch -> True
	]

	];


(* ::Subsection:: *)
(* Preview *)
(* ::Subsubsection:: *)
(* analyzeLadderPreview *)
analyzeLadderPreview[
	KeyValuePattern[{
			ResolvedOptions -> KeyValuePattern[{
				Output -> output_,
				PlotType -> plotType_
			}],
			Packet -> packet_
	}]
	] := Module[{newPacket, plot},

		(* If Preview not in Output, return Null. *)
		(* Consider converting output to always be a list in v1. *)
		If[Not[MatchQ[output, Preview]] && Not[MemberQ[output, Preview]],
			Return[<|Preview->Null|>]
		];

		(* Else, generate and return the plot. *)
		newPacket = stripAppendReplaceKeyHeads[packet];
		plot = PlotLadder[newPacket,PlotType->plotType];
		<|Preview->plot|>

	];


(* ::Subsection:: *)
(* InfoPeaksFields and Related Functions *)

peaksLookup={
	Object[Data,AgaroseGelElectrophoresis] -> {
		SampleElectropherogram -> SampleElectropherogramPeaksAnalyses,
		MarkerElectropherogram -> MarkerElectropherogramPeaksAnalyses,
		PostSelectionElectropherogram->PostSelectionPeaksAnalyses
	},
	Object[Data,Chromatography] -> {
		Absorbance -> AbsorbancePeaksAnalyses,
		Absorbance3D -> Absorbance3DPeaksAnalyses,
		SecondaryAbsorbance -> SecondaryAbsorbancePeaksAnalyses,
		Conductance -> ConductancePeaksAnalyses,
		Fluorescence -> FluorescencePeaksAnalyses,
		SecondaryFluorescence -> SecondaryFluorescencePeaksAnalyses,
		TertiaryFluorescence -> TertiaryFluorescencePeaksAnalyses,
		QuaternaryFluorescence -> QuaternaryFluorescencePeaksAnalyses,
		Scattering -> ScatteringPeaksAnalyses,
		DynamicLightScattering -> DynamicLightScatteringPeaksAnalyses,
		MultiAngleLightScattering22Degree -> MultiAngleLightScatteringAnalyses,
		RefractiveIndex -> RefractiveIndexPeaksAnalysis,
		Scattering -> ScatteringPeaksAnalyses,
		FIDResponse -> FIDResponsePeaksAnalyses,
		Charge -> ChargePeaksAnalyses
	},
	Object[Data,PAGE] -> {
		OptimalLaneIntensity -> LanePeaksAnalyses,
		OptimalLaneImage -> LanePeaksAnalyses,
		LowExposureLaneIntensity -> LanePeaksAnalyses,
		MediumLowExposureLaneIntensity -> LanePeaksAnalyses,
		MediumHighExposureLaneIntensity -> LanePeaksAnalyses,
		HighExposureLaneIntensity -> LanePeaksAnalyses,
		LowExposureLadderIntensity -> LanePeaksAnalyses,
		MediumLowExposureLadderIntensity -> LanePeaksAnalyses,
		MediumHighExposureLadderIntensity -> LanePeaksAnalyses,
		HighExposureLadderIntensity -> LanePeaksAnalyses,
		LowExposureGelImage -> LanePeaksAnalyses,
		MediumLowExposureGelImage -> LanePeaksAnalyses,
		MediumHighExposureGelImage -> LanePeaksAnalyses,
		HighExposureGelImage -> LanePeaksAnalyses,
		LowExposureGelImageFile -> LanePeaksAnalyses,
		MediumLowExposureGelImageFile -> LanePeaksAnalyses,
		MediumHighExposureGelImageFile -> LanePeaksAnalyses,
		HighExposureGelImageFile -> LanePeaksAnalyses
	},
	Object[Data,Western] -> {
		MassSpectrum -> MassSpectrumPeaksAnalyses
	},
	Object[Data,CapillaryGelElectrophoresisSDS] -> {
		ProcessedUVAbsorbanceData -> PeaksAnalyses,
		RelativeMigrationData -> RelativeMigrationPeaksAnalyses
	}
};

(* Searches Key, the field in Data Object where raw data is stored. *)
typeToPeaksFields[type:TypeP[]] := Part[
	Lookup[peaksLookup, type, {} ],
	All, 1
];

(* Searches Value, the field in Data object which points to the Analysis Object. *)
typeToPeaksSourceFields[type:TypeP[]] := Part[
	Lookup[peaksLookup, type, {} ],
	All, 2
];

(* Checks for valid user specified Key. *)
peaksFieldToPeaksSourceField[type:TypeP[], field:_Symbol] := Lookup[
	Lookup[peaksLookup, type, {} ],
	field, $Failed
];

(* Singleton,Null,or Special Input *)
infoPeaksField[in_] := infoPeaksField[in, Automatic];
infoPeaksField[Null, ___] := Null;
infoPeaksField[ObjectP[Object[Data, CapillaryGelElectrophoresisSDS]],Automatic] := RelativeMigrationData;

(* Link, ObjectRef, or Packet with 0 or 1 additional arguments. *)
infoPeaksField[in:LinkP[], rest___] := infoPeaksField[in[Object], rest];
infoPeaksField[in:ObjectReferenceP[], rest___] := infoPeaksField[in[Type], rest];
infoPeaksField[in:PacketP[], rest___] := infoPeaksField[in[Type], rest];

(* Listed Inputs *)
infoPeaksField[list_List] := Map[infoPeaksField, list];
infoPeaksField[list_List, thing_] := Map[infoPeaksField[#, Last[thing]]&, list];
infoPeaksField[list_List, list2_List] := MapThread[infoPeaksField[#1, #2] &, {list, list2}];

(* Primary Definitions *)
infoPeaksField[type:TypeP[ladderAnalysisTypes], symbol_] := Null;
infoPeaksField[type:TypeP[], Automatic] :=
		With[
			{peaksField = typeToPeaksFields[type]},
			If[peaksField =!= {}, First[peaksField], {}]
		];

infoPeaksField[type:TypeP[], symbol_] :=
		With[
			{peaksField = typeToPeaksFields[type]},
			If[!MemberQ[peaksField, symbol],
				Message[Warning::UnrecognizedPeaksField]; First[peaksField],
				symbol
			]
		];

(* ::Subsubsection:: *)
(*analyzeLadderUnits*)

(* typeUnits pulls specific fields-to-units rules from fields in DataObject. *)
typeUnits[object:ObjectReferenceP[]] := typeUnits[Download[object, Type]];
typeUnits[type:TypeP[]] := Cases[
	Lookup[
		LookupTypeDefinition[type],
		Fields
	],
	RuleDelayed[
		Rule[x_,{___,Rule[Units,y:Except[None]],___...}],
		x->y
	]
];

analyzeLadderUnits[in:(Null|$Failed)] := in;
analyzeLadderUnits[dataObject_] := Module[{xunit},
	(* xunit converts a field to a unit, according to rule specific to dataObject. *)
	xunit[field_] := First[field/.typeUnits[Download[dataObject, Type]], Null];
	Switch[dataObject,
		ObjectP[Object[Data,Chromatography]], {Nucleotide, xunit[Absorbance]},
		ObjectP[Object[Data,NMR]], {Null, xunit[NMRSpectrum]},
		ObjectP[Object[Data,PAGE]], {Nucleotide, Pixel},
		ObjectP[Object[Data,Western]], {Kilo Dalton, xunit[MassSpectrum]},
		ObjectP[Object[Data,CapillaryGelElectrophoresisSDS]], {Kilo Dalton, RRT},
		ObjectP[Object[Data,AgaroseGelElectrophoresis]], {Kilo Dalton, RRT},
		_, (Message[Warning::UnknownUnits,dataObject]; {Null,Null})
	]
];

